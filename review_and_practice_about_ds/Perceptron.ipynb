{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- based on : https://datascienceschool.net/view-notebook/342b8e2ecf7a4911a727e6fe97f4ab6b/\n",
    "- based on: http://scikitlearn.org/stable/modules/generated/sklearn.linear_model.Perceptron.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Perceptron\n",
    "- is another simple algorithm suitable for large scale learning:\n",
    "    - It does not require a learning rate.\n",
    "    - It is not regularized(penalized)\n",
    "    - It updates its model only on mistakes.\n",
    "- The last characteristic implies that the Perceptron is slightly faster to trian than SGD with the hinge loss and that the resulting models are sparser."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 퍼셉트론\n",
    "- 퍼셉트론은 가장 오래되고 단순한 형태의 판별 함수 기반 예측 모형 중 하나이다.\n",
    "- 퍼셉트론은 입력 x=(x_1,x_2,..,x_m)dp eogo 1 또는 -1의 값을 가지는 종속 변수를 출력하는 비선형 함수이다. 1을 포함하는 입력 요소에 대해 가중치를 곱한 값이 판별 함수가 된다.\n",
    "- 판별 한수 값이 활성화 함수를 지나면 분류 결과를 나타내는 출력이 생성된다.\n",
    "- `y=a(f(x)) = a(np.dot(w.T,x))`\n",
    "- 퍼셉트론은 활성화 함수로 다음과 같은 Heviside step function을 가진다.\n",
    "    - y = a(z) = if z<0: return -1 ; else: return 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 퍼셉트론 손실 함수\n",
    "- 퍼셉트론은 독립 변수로부터 종속 변수를 예측하는 모형이므로 예측 오차를 최소화하는 가중치를 계산해야 한다. (RSS와 유사하다.)\n",
    "- 가중치에 따라 달리지는 전체 예측 오타는 i번째 개별 데이터에 대한 손실의 합으로 표현할 수 있다.\n",
    "- 손실은 실제값과 예측값의 차이를 나타내는 함수이다.\n",
    "- 나타내는 함수 방식이 RSS와는 다른데, 이유는 판별 함수에서 이진 분류로 -1,1을 사용하는데에 이유가 있다.\n",
    "- `L(y_hat{i} , y{i}) = max(0,-y_hat{i}*y{i})`\n",
    "- 위의 방정식은 간단히 해석해보면(간단하다) 예측이 맞으면 0, 틀리면 1이 된다. 즉 틀리면 Loss Function이 당연한 이야기지만 더 커지는 것이다.\n",
    "- 위의 식은 데이터에 대입해보면, 어자피 예측이 맞은 값들은 0이 되어 사라지고, 틀린 값들만 남게 된다. 즉 오분류된 데이터의 집합의 합이 된다.\n",
    "- 이때 Heaviside step function을 그대로 적용하면 미분이 불가능하기 때문에 활성화 함수를 적용하기 전인 판별 함수 `f(x) = np.dot(w.T,x)`를 사용하여 미분한다.\n",
    "- `E(w) = -sigma(missclassification){np.dot(w.T,x_i,y_i)}`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 가중치 계산\n",
    "- 손실 함수를 최소화하는 가중치를 찾기 위해 손실 함수를 가중치로 미분하여 gradient를 구한다.\n",
    "- `dE / dw = -sigma(misclassification){x_i * y_i}`\n",
    "- gradient descent 방법을 사용하면 다음과 같이 가중치를 구할 수 있다.\n",
    "- `w{k+1} = w{k} + step_size * sigma(missclassification){x_i * y_i}`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SGD(Stochastic Grandient Descent) -- optimization method\n",
    "- 퍼셉트론은 일반적인 gradient descent 방법이 아닌 SGD 최적화 방법을 사용한다.\n",
    "- SGD최적화 방법은 정확한 gradient 대신 일부 표본 데이터만 사용한 추정치를 이용하는 방법이다.\n",
    "- 퍼셉트론은 가장 극단적인 경우로 minibatch size=1 ( minibatch size 는 추정에 사용하는 표본 데이터의 갯수를 의미한다.) \n",
    "- 즉, 한번에 하나의 오분류된 데이터만을 이용하여 가중치를 조정한다.\n",
    "- `w{k+1} = w{k} + step_size * x{i} * y{i}`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 퍼셉트론의 단순 모형 : `Perceptron`\n",
    "- 퍼셉트론의 더 다양한 옵션 인수를 제공하는 : `SGDClassifier`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_iris\n",
    "iris = load_iris()\n",
    "X,y = iris.data,iris.target\n",
    "X = X[:100,0:2]\n",
    "y=y[:100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/hskimim/anaconda3/lib/python3.6/site-packages/sklearn/linear_model/stochastic_gradient.py:128: FutureWarning: max_iter and tol parameters have been added in <class 'sklearn.linear_model.perceptron.Perceptron'> in 0.19. If both are left unset, they default to max_iter=5 and tol=None. If tol is not None, max_iter defaults to max_iter=1000. From 0.21, default max_iter will be 1000, and default tol will be 1e-3.\n",
      "  \"and default tol will be 1e-3.\" % type(self), FutureWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Perceptron(alpha=0.0001, class_weight=None, eta0=1.0, fit_intercept=True,\n",
       "      max_iter=None, n_iter=None, n_jobs=1, penalty=None, random_state=0,\n",
       "      shuffle=True, tol=None, verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.linear_model import Perceptron\n",
    "perceptron = Perceptron().fit(X,y)\n",
    "perceptron"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/hskimim/anaconda3/lib/python3.6/site-packages/sklearn/linear_model/stochastic_gradient.py:128: FutureWarning: max_iter and tol parameters have been added in <class 'sklearn.linear_model.perceptron.Perceptron'> in 0.19. If both are left unset, they default to max_iter=5 and tol=None. If tol is not None, max_iter defaults to max_iter=1000. From 0.21, default max_iter will be 1000, and default tol will be 1e-3.\n",
      "  \"and default tol will be 1e-3.\" % type(self), FutureWarning)\n",
      "/home/hskimim/anaconda3/lib/python3.6/site-packages/sklearn/linear_model/stochastic_gradient.py:128: FutureWarning: max_iter and tol parameters have been added in <class 'sklearn.linear_model.perceptron.Perceptron'> in 0.19. If both are left unset, they default to max_iter=5 and tol=None. If tol is not None, max_iter defaults to max_iter=1000. From 0.21, default max_iter will be 1000, and default tol will be 1e-3.\n",
      "  \"and default tol will be 1e-3.\" % type(self), FutureWarning)\n",
      "/home/hskimim/anaconda3/lib/python3.6/site-packages/sklearn/linear_model/stochastic_gradient.py:128: FutureWarning: max_iter and tol parameters have been added in <class 'sklearn.linear_model.perceptron.Perceptron'> in 0.19. If both are left unset, they default to max_iter=5 and tol=None. If tol is not None, max_iter defaults to max_iter=1000. From 0.21, default max_iter will be 1000, and default tol will be 1e-3.\n",
      "  \"and default tol will be 1e-3.\" % type(self), FutureWarning)\n",
      "/home/hskimim/anaconda3/lib/python3.6/site-packages/sklearn/linear_model/stochastic_gradient.py:128: FutureWarning: max_iter and tol parameters have been added in <class 'sklearn.linear_model.perceptron.Perceptron'> in 0.19. If both are left unset, they default to max_iter=5 and tol=None. If tol is not None, max_iter defaults to max_iter=1000. From 0.21, default max_iter will be 1000, and default tol will be 1e-3.\n",
      "  \"and default tol will be 1e-3.\" % type(self), FutureWarning)\n",
      "/home/hskimim/anaconda3/lib/python3.6/site-packages/sklearn/linear_model/stochastic_gradient.py:128: FutureWarning: max_iter and tol parameters have been added in <class 'sklearn.linear_model.perceptron.Perceptron'> in 0.19. If both are left unset, they default to max_iter=5 and tol=None. If tol is not None, max_iter defaults to max_iter=1000. From 0.21, default max_iter will be 1000, and default tol will be 1e-3.\n",
      "  \"and default tol will be 1e-3.\" % type(self), FutureWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([1.  , 1.  , 0.95, 0.85, 1.  ])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "kfold = KFold(5)\n",
    "cross_val_score(perceptron,X,y,cv=kfold)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report,confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[49,  1],\n",
       "       [ 2, 48]])"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "confusion_matrix(y,perceptron.predict(X))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.96      0.98      0.97        50\n",
      "          1       0.98      0.96      0.97        50\n",
      "\n",
      "avg / total       0.97      0.97      0.97       100\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(y,perceptron.predict(X)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import SGDClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/hskimim/anaconda3/lib/python3.6/site-packages/sklearn/linear_model/stochastic_gradient.py:128: FutureWarning: max_iter and tol parameters have been added in <class 'sklearn.linear_model.stochastic_gradient.SGDClassifier'> in 0.19. If both are left unset, they default to max_iter=5 and tol=None. If tol is not None, max_iter defaults to max_iter=1000. From 0.21, default max_iter will be 1000, and default tol will be 1e-3.\n",
      "  \"and default tol will be 1e-3.\" % type(self), FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "sgdc = SGDClassifier().fit(X,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/hskimim/anaconda3/lib/python3.6/site-packages/sklearn/linear_model/stochastic_gradient.py:128: FutureWarning: max_iter and tol parameters have been added in <class 'sklearn.linear_model.stochastic_gradient.SGDClassifier'> in 0.19. If both are left unset, they default to max_iter=5 and tol=None. If tol is not None, max_iter defaults to max_iter=1000. From 0.21, default max_iter will be 1000, and default tol will be 1e-3.\n",
      "  \"and default tol will be 1e-3.\" % type(self), FutureWarning)\n",
      "/home/hskimim/anaconda3/lib/python3.6/site-packages/sklearn/linear_model/stochastic_gradient.py:128: FutureWarning: max_iter and tol parameters have been added in <class 'sklearn.linear_model.stochastic_gradient.SGDClassifier'> in 0.19. If both are left unset, they default to max_iter=5 and tol=None. If tol is not None, max_iter defaults to max_iter=1000. From 0.21, default max_iter will be 1000, and default tol will be 1e-3.\n",
      "  \"and default tol will be 1e-3.\" % type(self), FutureWarning)\n",
      "/home/hskimim/anaconda3/lib/python3.6/site-packages/sklearn/linear_model/stochastic_gradient.py:128: FutureWarning: max_iter and tol parameters have been added in <class 'sklearn.linear_model.stochastic_gradient.SGDClassifier'> in 0.19. If both are left unset, they default to max_iter=5 and tol=None. If tol is not None, max_iter defaults to max_iter=1000. From 0.21, default max_iter will be 1000, and default tol will be 1e-3.\n",
      "  \"and default tol will be 1e-3.\" % type(self), FutureWarning)\n",
      "/home/hskimim/anaconda3/lib/python3.6/site-packages/sklearn/linear_model/stochastic_gradient.py:128: FutureWarning: max_iter and tol parameters have been added in <class 'sklearn.linear_model.stochastic_gradient.SGDClassifier'> in 0.19. If both are left unset, they default to max_iter=5 and tol=None. If tol is not None, max_iter defaults to max_iter=1000. From 0.21, default max_iter will be 1000, and default tol will be 1e-3.\n",
      "  \"and default tol will be 1e-3.\" % type(self), FutureWarning)\n",
      "/home/hskimim/anaconda3/lib/python3.6/site-packages/sklearn/linear_model/stochastic_gradient.py:128: FutureWarning: max_iter and tol parameters have been added in <class 'sklearn.linear_model.stochastic_gradient.SGDClassifier'> in 0.19. If both are left unset, they default to max_iter=5 and tol=None. If tol is not None, max_iter defaults to max_iter=1000. From 0.21, default max_iter will be 1000, and default tol will be 1e-3.\n",
      "  \"and default tol will be 1e-3.\" % type(self), FutureWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([1.  , 1.  , 0.95, 1.  , 0.95])"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "kfold = 5\n",
    "cross_val_score(sgdc,X,y,cv=kfold)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
