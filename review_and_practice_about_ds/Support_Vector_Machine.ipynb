{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- based on : http://scikit-learn.org/stable/modules/svm.html\n",
    "- based on : https://datascienceschool.net/viewnotebook/6c6d450cb2ee49558856fd924b326e00/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 서포트 벡터 머신\n",
    "- 퍼셉트론은 가장 단순하고 빠른 판별 함수 기반 분류 모형이지만 판별 경계선이 유니크하게 존재하지 않는다는 특징이 있다. (쉽게 말하면 분류는 되는데 경계선이 이리 저리 움직여서 수렴하지 않는다는 뜻이다.)\n",
    "- 서포트 벡터 모신은 퍼셉트론 기반의 모형에 가장 안정적인 판별 경계선을 찾기 위한 제한 조건을 추가한 모형이라고 볼 수 있다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 서포트와 마진\n",
    "- 서포트 벡터 머신에서도 종속 변수는 이진 분류되며 -1,1값으로 나뉘어지게 된다.\n",
    "- 독립 변수(데이터) 중에서 라벨값이 1인 것이 있고 -1이 되는 것이 있다.(나뉘어진다.)\n",
    "- 판별 함수 모형에서 직선인 판별 함수 f(x)는 다음과 같은 수식으로 나타낼 수 있다.\n",
    "- `f(x) = np.dot(w.T,x) - w_0`(직선의 방정식)\n",
    "- 판별 함수의 정의에 따라 y값이 1인 판별 함수 값은 양수가 되고, 반대는 음수가 된다.\n",
    "- 종속 변수(라벨)값이 1인 데이터 중에서 판별 함수의 값이 가장 작은 데이터를 x+라고 하고 반대로 종속 변수(라벨)값이 -1인 데이터 중에서 판별 함수의 값이 가장 큰 데이터를 x-라고 하자.\n",
    "- 이 데이터들은 각각의 클래스에 속한 데이터 중에서 가장 경계선에 가까이 붙어있는 최전방(most front) 데이터 들이다. 이러한 데이터를 서포트 혹은 서포트 벡터라고 한다. 물론 이 서포트에 대해서도 판별 함수의 부호 조건(바로 위에서 언급했었다)은 만족되어야 한다.\n",
    "- 서포트에 대한 판별 함수의 값은 부호 조건만 지키면 어떤 값이 되어도 괜찮다.(양수 아니면 음수 조건만 지키면 된다는 말씀) 따라서 다음과 같은 조건을 만족하도록 판별 함수를 구한다.\n",
    "- `f(positive_support) = np.dot(w.T,positive_support)-w_0 = 1`\n",
    "- `f(negative_support) = np.dot(w.T,negative_support)-w_0 = -1`\n",
    "- 위에서 언급했다시피 서포트의 정의는 데이터 중에서 양,음 각각하여 판별 함수와의 거리가 가장 짧은 입력 변수(데이터)를 의미한다고 하였다. 따라서 아래의 등식이 성립한다.\n",
    "- `f(positive_support) = np.dot(w.T,positive_support)-w_0 >= 1`\n",
    "- `f(negative_support) = np.dot(w.T,negative_support)-w_0 <= -1`\n",
    "- 판별 경계선과 점 사이의 거리는 양변에 dist(w)를 나눠주게 되면 된다.\n",
    "- `(np.dot(w.T,positive_support)-w_0) / dist(w) = 1/dist(w)`\n",
    "- `(np.dot(w.T,negative_support)-w_0) / dist(w) 볼 수 있다. 그런데 위해서 정한 스케일링에 의해서 마진은 다음과 같이 정리된다.\n",
    "- `(np.dot(w.T,positive_support)-w_0) / dist(w) + (np.dot(w.T,negative_support)-w_0) / dist(w) = 2/dist(w)`\n",
    "- 마진 값이 최대가 되는 경우는 dist(w)(편의상 square한다.)가 최소가 되는 경우와 같다. 즉 다음과 같은 목적함수를 최소화하면 된다.\n",
    "- `L = 1/2 * dist(w)^2 = 1/2 * np.dot(w.T,w)`(1/2은 계산의 편리성을 위한 것이다.)\n",
    "- 또한 모든 표본 데이터에 대해 분류는 제대로 되어야 하므로 모든 데이터 x{i},y{i}에 대해 다음 조건을 만족해야 한다. 위에서 스케일링을 사용하여 모든 데이터에 대해 판별함수가 1보다 크거나 -1보다 작게 만들었다는 점을 이용한다.(바로 위에서 다뤘다. 부등식 성립 조건은 서포트 벡터가 1과 -1인 것으로 가정했다는 것에서 기반한다.)\n",
    "- 위에서 말이 길었는데, 간단하게 말하면 실제 종속 변수(라벨)과 판별 함수를 곱해주는 것이다. 옳게 예측을 하게 되면 양,음이 각각 나오는 종속 변수(라벨)에 같이 판별 함수 또한 양,음으로 매칭되기 때문에, 항상 0보다 크거나 작은 부등식이 성립한다.(분류가 제대로 되었을 경우에)\n",
    "- `y_i * (np.dot(w.T,x_i) - w_0)-1 >= 0`\n",
    "- 위의 등식이 부등식 제한 조건이고, 잊지 않았겠지만, 우리는 비용함수를 최소화시켜주는 가중치 요소를 미분해주어야 한다. 즉, 라그랑지 승수를 적용해준다.(KKT-condition)\n",
    "- `L = 1/2*np.dot(w.T,w) - sum(a(y_i * (np.dot(w.T,x_i) - w_0)-1)`\n",
    "- 위의 최적화 문제를 풀어 w,w_0,a 를 구하면 판별 함수를 얻을 수 있다.\n",
    "- KKT-condition에서 유도할 수 있다시피, hyper-parameter or inequality condition =0 조건이기 때문에, 등식 속에 있는`np.dot(w.T,x_i) - w_0)-1 !=0`인 경우 즉, 서포트 벡터가 아닌 경우에는 hyper-parmeter, a가 0이 된다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dual Form\n",
    "- 최적화 조건은 목적 함수 L을 w,w_0로 미분한 값이 0이 되어야 한다는 것이다.\n",
    "- 질문: a는 어디로 갔는가? : 위의 마지막에서 언급했다시피, 서포트 벡터가 아니면 하이퍼 모수는 0이 된다(depend on KKT-condition) 따라서 `판별 함수를 정하는 모수는 결국 서포트 벡터에게 달려있다. 다른 입력 변수(데이터)들을 영향력이 없다`가 서포트 벡터 머신의 메인 이슈이다.\n",
    "- `dL/dw = 0, dL/dw_0 = 0`이 두 미분식을 풀어서 정리한 후에, 원래의 목적 함수에 대입하여 모수들을 없애준다. ( 중간 과정이 너무 복잡해 생략한다...)\n",
    "- `L = sum(a_i) -1/2 * sum(sum(a_i*a_j*y_i*y_j*np.dot(x_i.T,x_j)))`\n",
    "- y_i*y_j*np.dot(x_i.T,x_j) 는 스칼라이다.\n",
    "- 위의 방정식에서 a는 `sum(a_i * y_i) = 0`을 만족하낟.\n",
    "- 이 문제는  w 를 구하는 문제가 아니라  a 만을 구하는 문제로 바뀌었으므로 dual form 이라고 한다. dual form 문제는 수치적으로 Box 제한 조건이 있는 이차 프로그래밍 문제(QP; quadratic programming)가 되므로 효율적으로 풀 수 있다.\n",
    "- dual form 문제를 풀어서 목적 함수를 최소화하는 하이퍼 모수를 구하면 예측 모형을 \n",
    "- `f(x) = w^T x - w_0 = \\sum_{i=1}^N a_i y_i x_i^T x - w_0` 과 같이 쓸 수 있다. 이해하기 힘들 수도 있지만, 여기서 포인트는 a가 a_i*y_i*(x_i.T)로 바뀌었다는 것이다.\n",
    "- `f(x) = a^+ x^T x^+ - a^- x^T x^- - w_0`\n",
    "- 위의 등식에서 x^T x^는 내적이고 이 내적은 코사인 유사도를 의미하기 때문에 결국 두 서포트 벡터와의 유사도를 측정해서 값이 큰 쪽으로 판별하게 된다.\n",
    "- 즉, margin safety를 크게 하는 쪽으로 판별 함수를 만들게 되는 것은 위와 같다.\n",
    "- 듀얼 폼을 형성하는 이유는 이차 형식으로 만들어서 컴퓨팅을 편하게 하기 위함이다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_iris\n",
    "iris = load_iris()\n",
    "X,y = iris.data,iris.target\n",
    "X = X[:100,:2]\n",
    "y = y[:100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC\n",
    "model = SVC(kernel = 'linear',C = 1e10).fit(X,y)\n",
    "# kernel , C attribute는 후에 따로 설명하도록 하겠다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([2, 2], dtype=int32)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.n_support_\n",
    "# 각 클래스의 서포트의 개수"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([36, 41, 57, 84], dtype=int32)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.support_\n",
    "# 각 클래스의 서포트의 인덱스"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[5.5, 3.5],\n",
       "       [4.5, 2.3],\n",
       "       [4.9, 2.4],\n",
       "       [5.4, 3. ]])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.support_vectors_\n",
    "# 각 클래스의 서포트의 x 값"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 1, 1])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y[model.support_]\n",
    "# label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([35.32689299])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_new = [10, 2]\n",
    "model.decision_function([x_new])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 슬랙 변수\n",
    "- 만약 데이터가 직선인 판별 경계선으로 나누어지지 않는, 즉, 선형 분리(linear separable)가 불가능한 경웽는 다음과 같이 슬랙 변수를 사용하여 개별적인 오차를 허용할 수 있다.\n",
    "- 원래 판별 함수 값은 라벨이 양수일 때 1보다 커야 하고, 음수일 때는 -1보다 작아야 한다는 조건을 걸어놓았었다.\n",
    "- 이 때, 양수인 슬랙 변수를 사용하면 이 조건을 완화할 수 있다.\n",
    "- 슬랙 변수가 양수여야 하는 이유는, 굉장히 직관적인데, 기억이 날지는 모르겠지만, 위에서 목적 함수 즉, 비용 함수를 계산할 때, 종속 변수 데이터와 판별 함수를 곱을 해주었다. 데이터가 옳은 경우는 이래나저래나 1 보다 크거나 같은 수가 나와서 라그랑지 승수에 조건으로 삽입해주었었다. 기억이 안나면 위로 올라가길 바란다.\n",
    "- 슬랙 변수가 양수가 되게 되면 오른쪽에 위치만(1-slack , -1+slack)보게 되면, 최적화 목적 함수가 좀더 나이브해지는 것을 알 수 있다.\n",
    "- 위에서 나왔던, 특성 C는 슬랙 변수의 패널티 하이퍼 모수로써 크면 클수록 엄격해진다."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
