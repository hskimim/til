{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "from : https://machinelearningmastery.com/difference-between-a-batch-and-an-epoch/\n",
    "- 본문은 영어인데, 공부도 할겸, 가독성도 높일겸 한글로 번역해서 쓰겠다.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## overview\n",
    "- Gradient Descent\n",
    "- what is a Batch?\n",
    "- what is a Epoch?\n",
    "- what is the diff between batch and epoch? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gradient Descent\n",
    "- 머신러닝과 딥러닝의 인공신경망(주로 이곳에 많이 쓰인다.) 에 사용되는 최적화 알고리즘이다.\n",
    "- 어떠한 측정 기준을(오차 함수나 패널티 함수에 기반하는데, mse or logarithmic loss등이 사용된다.) 기반으로 함수 내부의 최적의 파라미터들을 찾는데에 사용된다.\n",
    "- 최적화는 러닝 프로세스의 절차와 같은 맥락이다. gradient descent에서 gradient는 오차 함수의 기울기를 의미하며, 최적화 프로세스는 오차 함수 값의 최소화 단계를 의미한다.\n",
    "- 이 알고리즘은 반복적이다. 이 의미는 이 알고리즘 프로세스는 모델 파라미터 조정을 위해서 여러번 반복해서 조정 작업을 하면서 발생한다는 것이다.\n",
    "- Gradient Descent 알고리즘 중에 Stochastic Gradient Descent 라는 알고리즘이 있는데, 이 방법론은 데이터의 크기가 너무나도 커서 모든 데이터 셋에 대해 알아보고 파라미터 튜닝이 불가능하다고 느껴질 때, 싱글 데이터 포인트에 대한 데이터 튜닝을 실시하는 것을 의미한다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Batch\n",
    "- 어떠한 알고리즘을 사용해서 함수 내부의 파라미터를 조정하게 될 때, 조정하는 과정에서 작업하게 되는 샘플의 갯수를 의미한다.\n",
    "- Batch를 다음과 같이 생각해보자\n",
    "    - 1개 이상의 샘플에 대한 for 반목문이 돌아가고 예측이 된다. 그리고 Batch가 끝나게 되면 답안지(라벨 트레이닝 데이터)와 비교를 하게 되며, 이때 오차 함수의 페널티 값이 나온다. \n",
    "- Batch Gradient Descent : Batch size = Size of Training Set\n",
    "- Stochastic Gradient Descent : Batch Size = 1\n",
    "- Mini-Batch Gradient Descent : 1 < Batch Size < Size of Training set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Epoch\n",
    "- 최적화 알고리즘을 수행하면서 우리가 트레이닝 할 데이터 세트를 전체적으로 몇 번 볼 것이냐에 대한 하이퍼 파라미터이다.\n",
    "- 하나의 Epoch는 1개 이상의 Batch로 구성되어 있다. 예로 들어서 Batch Gradient Descent 는 1개의 배치가 전체 트레이닝 셋의 사이즈와 같게끔 설정되어 있는 방법론이기 때문에 에포크와 배치 사이즈가 같다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What is the Diff between Batch and Epoch?\n",
    "- 배치 사이즈는 모델이 업데이트 되기 전에 수행되는 샘플의 갯수이다.\n",
    "- 에포크 숫자는 전체 트레이닝 셋이 모두 인식? (work through)되는 횟수이다.\n",
    "- 배치 사이즈는 1보다는 커야 하며 전체 트레이닝 데이터 셋 샘플의 갯수보다 작거나 같아야 한다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Worked Example\n",
    "- 여태까지 말로 한 것을 쉽게 예시로 들어보자\n",
    "- 우리가 데이터 셋 200개를 가지고 있다고 생각하고 batch size = 5 , 1000 epochs로 하이퍼 파라미터를 설정했다고 하자\n",
    "- 이 의미는 데이터 셋트를 각각 5개의 샘플이 되어 40 batches로 나누고 (200/5) 이 모델의 가중치는 각각 5개의 샘플의 러닝이 끝나게 되면 업데이트 된다.\n",
    "- 이 의미는 1개의 에포크가 40개의 배치 사이즈와 40번의 모델 업데이트를 의미한다.\n",
    "- 1000 에포크는 전체 트레이닝 데이터 세트를 1000번 노출? 인식? 한다는 의미이다.\n",
    "- 결과적으로 40,000 배치를 하게 된다.\n",
    "    - 200개 트레이닝 세트\n",
    "    - 배치 사이즈 5\n",
    "    - 200 / 5 = 40 개의 샘플 데이터로 나뉘게 되고\n",
    "    - 40개의 샘플 데이터 각각 가중치가 오차 함수에 의해 업데이트 되며\n",
    "    - 에포크 횟수가 1000이기 때문에 40개 샘플 데이터와 40번의 가중치 업데이트가\n",
    "    - 총 1000번 일어나기 때문에 결국 러닝 카운트는 40*1000 = 40,000번이다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Summary\n",
    "- Stochastic gradient descetn is an iterative leawrning algorithm that uses a training dataset to update a model\n",
    "- the batch size is a hypeparameter of gradient descent that controls the number of training sazmples to work through before the model's internal parameters are updated\n",
    "- the number of epoch is a hyperpareameter of gradient descent that controls the number of complete passeds through the training data set."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
