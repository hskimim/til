{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import statsmodels.api as sm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def entropy(prop):\n",
    "    return (-1*prop*np.log2(prop)) + (-1*(1-prop)*np.log2(1-prop))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "entropy(0.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 엔트로피의 성질\n",
    "- 엔트로피의 최대값은 이산 확률변수의 클래스의 갯수에 따라 달라진다. 만약 이산 확률 변수가 가질 수 있는 클래스가 2**K 개이고 이산 확률 변수가 가질 수 있는 엔트로피의 최대값은 각 클래스가 모두 같은 확률을 가지는 때이다. 이 때 엔트로피의 값은 K이다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 엔트로피와 정보량\n",
    "- 엔트로피는 확률변수가 담을 수 있는 정보량을 의미한다.\n",
    "- 엔트로피가 0이면 확률변수는 결정론적이므로 확률 변수의 표본값이 변화하지 않는다. 즉 정보량이 없다. 따라서 확률 변수의 표본값을 관측한다고 해도 우리가 얻을 수 있는 정보는 없다. \n",
    "- 만약 문서에서 각 알파벳이 나올 확률이 다음과 같다고 가정하자(Dictvectorizer 기반)\n",
    "    - {1/2,1/4,1/8,1/8}\n",
    "- 이 4개의 문자를 그냥 보내기 위해서는 2자리수의 이진수를 사용해야 한다. 00,01,10,11\n",
    "- 이 문자의 엔트로피를 계산해보자"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def entropy2(prop1,prop2,prop3,prop4):\n",
    "    assert (prop1 + prop2 + prop3 + prop4) == 1\n",
    "    return (-1*prop1*np.log2(prop1)) + (-1*prop2*np.log2(prop2)) +\\\n",
    "(-1*prop3*np.log2(prop3)) + (-1*prop4*np.log2(prop4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.75"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "entropy2(1/2,1/4,1/8,1/8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "위와 같이 해당 문자들이 줄수있는 정보량은 최대 2에서 1.75에 해당한다. 따라서 원래의 이진법상에 해당하는 2가 아닌 1.75의 정보량이기 때문에 1000개의 문자를 보내야 한다면 2000개가 아닌 1750개만 보내도 된다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 표본 데이터가 주어진 경우\n",
    "- 확률 변수 모형 즉, 이론적인 확률 밀도(질량)함수가 주어진 것이 아니라 데이터가 주어졌다면 확률 질량 함수를 추정하여서 엔트로피를 계산한다.\n",
    "- 위에서 계속 다뤘다시피 엔트로피 계산을 위해서는 P(y)를 알아야 한다. 이를 어떻게 알 수 있을까 prior이다. \n",
    "- 예로 들어 데이터가 모두 80개이고 클래스 별로 40,40개가 각각 있으면? 0.5,0.5로 추정할 수 있다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 조건부 엔트로피\n",
    "- 조건부 엔트로피는 상관관계가 있는 두 확률변수 X,Y가 있고 X의 값을 안다면 Y의 확률 변수가 가질 수 있는 정보의 양을 뜻한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Y=0</th>\n",
       "      <th>Y=1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>X=0</th>\n",
       "      <td>30</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>X=1</th>\n",
       "      <td>10</td>\n",
       "      <td>30</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     Y=0  Y=1\n",
       "X=0   30   10\n",
       "X=1   10   30"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "condi_df = pd.DataFrame([[30,10],[10,30]])\n",
    "condi_df.columns=['Y=0','Y=1']\n",
    "condi_df.index=['X=0','X=1']\n",
    "condi_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_prior = sum(condi_df.iloc[:,0])/(sum(condi_df.iloc[:,0]) + sum(condi_df.iloc[:,1]))\n",
    "y_prior"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "condi_entropy1 = entropy(3/4).copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "condi_entropy2 = entropy(1/4).copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8112781244591328"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_prior * condi_entropy1 + (1-y_prior)*condi_entropy2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 수학적 정의는 너무 복잡해보인다 위에서 제시한 함수에서도 수학적 정의를 알아채기 힘들 수도 있을 정도로 힘든데, 직관적으로 생각해보자.\n",
    "- 이산형 공식을 알면 연속형 공식도 자연히 알게 되기 때문에 여기서는 이산형 문제만 다루도록 하겠다.\n",
    "- 데이터의 수(prior)를 일단 구한다. 위의 식에서는 y_prior로 계산하게 된다. 즉, 클래스0에 속하는 데이터의 수와 클래스1에 속하는 데이터의 수에 ratio를 계산한다.\n",
    "- 그 다음에는 어떤 데이터냐에 대한 문제로 들어간다. 이 단계로 인해서 조건부 엔트로피라고 불리는 건데, 데이터의 특성이 0이냐 1이냐로 나뉘게 되는 것이다. 데이터의 특성이 0일때의 엔트로피를 계산하고 데이터의 특성이 1일때의 엔트로피를 각각 계산한다. 이렇게 되면 이 자체가 조건부 엔트로피가 된다.\n",
    "- 마지막으로 prior에 가중 평균을 내주면 된다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 크로스 엔트로피\n",
    "- 주로 분류 문제의 목표값 분포와 예측값 분포를 비교하는데 사용된다.\n",
    "- 예를 들어서 이진 분류문제에서는 Y는 0 또는 1이라는 값만 가질 수 있다. 또 예측값 y_hat 은 Y=1일 확률이 theta라고 하자. 모수 theta로 대표되는 Y의 분포와 모수로 theta_hat으로 대표되는 Y_hat의 분포라고 하면 크로스 엔트로피는 다음과 같이 추정된다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- N : 총 데이터의 수\n",
    "- theta : 목표값의 모수\n",
    "- theta_hat : 예측값의 모수"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_iris\n",
    "iris = load_iris()\n",
    "dfX = pd.DataFrame(iris.data,columns=iris.feature_names)\n",
    "dfy = pd.DataFrame(iris.target,columns=['y'])\n",
    "df = pd.concat([dfX,dfy],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df[df['y']!=2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfX = df.iloc[:,0:4]\n",
    "dfy = df.iloc[:,-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((100,), 100)"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "lda = LinearDiscriminantAnalysis()\n",
    "model = LinearDiscriminantAnalysis().fit(dfX,dfy)\n",
    "y_pred = model.predict(dfX)\n",
    "y_true = dfy\n",
    "y_true.shape,len(y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "prop_feature = sum(y_true.values)/len(y_true.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prop_predict = sum(y_pred)/len(y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cross_entropy(theta,theta_hat):\n",
    "    return -theta*np.log2(theta_hat)-(1-theta)*np.log2(1-theta_hat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.1257693834979823"
      ]
     },
     "execution_count": 114,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cross_entropy(0.5,0.3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 쿨백-라이블러 발산\n",
    "- 크로스 엔트로피의 개량판이다.\n",
    "- 목표 분포와 예측 분포의 차이를 정량화하는 방법의 하나이다.\n",
    "- cross-entropy - object distribution's entropy\n",
    "- 쿨백-라이블러 발산은 크로스 엔트로피에서 대상이 되는 분포의 엔트로피는 뺀 값이므로 상대 엔트로피라고도 한다. 그 값은 항상 양수이며 두 확률 분포가 완전히 같은 경우에만 0이 된다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`Kullback-Leibler divergence = -p(y)*log2(q(y))-q(y)*log2(q(y)) - p(y)`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 지니 불순도 (Gini impurity)\n",
    "- 엔트로피와 유사한 개념으로 지니 불순도라는 것이 있다.\n",
    "- 지니 불순도는 엔트로피처럼 확률 분포가 어느쪽에 치우쳐있는가를 재는 척도이다.\n",
    "- 로그를 사용하지 않기 때문에 computation 문제에서 부담이 적다."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
